{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo Um Algoritmo Para Rede Neural Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização com Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) é uma versão de Gradient Descent, onde em cada passagem para a frente, obtemos um lote de dados com amostras aleatórias do conjunto de dados total. Aqui onde entra em cena o batch_size. Esse é o tamanho do lote. Idealmente, todo o conjunto de dados seria alimentado na rede neural em cada passagem para a frente, mas na prática isso acaba não sendo possível, devido a restrições de memória. SGD é uma aproximação de Gradient Descent, quanto mais lotes processados pela rede neural, melhor será a aproximação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma implementação do SGD envolve:\n",
    "\n",
    "1. Gerar lotes de dados de amostras aleatórias do conjunto de dados total.\n",
    "\n",
    "2. Executar a rede para frente (Forward Pass) e para trás (Backward pass) para calcular o gradiente (com dados de (1)).\n",
    "\n",
    "3. Aplicar a atualização de descida do gradiente.\n",
    "\n",
    "4. Repitir as etapas 1-3 até a convergência ou o loop for parado por outro mecanismo (como o número de épocas, por exemplo).\n",
    "\n",
    "Se tudo correr bem, a perda da rede vai diminuindo, indicando pesos e bias mais úteis ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuronio:\n",
    "    \"\"\"\n",
    "    Classe base para os nós da rede.\n",
    "\n",
    "    Argumentos:\n",
    "\n",
    "        \"nodes_entrada\": Uma lista de nós com arestas para este nó.\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes_entrada = []):\n",
    "        \"\"\"\n",
    "        O construtor do nó (é executado quando o objeto é instanciado). \n",
    "        Define propriedades que podem ser usadas por todos os nós.\n",
    "        \"\"\"\n",
    "        # Lista de nós com arestas para este nó.\n",
    "        self.nodes_entrada = nodes_entrada\n",
    "        \n",
    "        # Lista de nós para os quais este nó gera saída.\n",
    "        self.nodes_saida = []\n",
    "        \n",
    "        # O valor calculado por este nó. É definido executando o método forward().\n",
    "        self.valor = None\n",
    "        \n",
    "        # Este objeto é um dicionário com pares chaves/valor entre {} \n",
    "        # As chaves (keys) são os inputs para este nó e o valores (values) são as paciais deste nó em relação ao input.\n",
    "        self.gradientes = {}\n",
    "        \n",
    "        # Configuramos este nó como um nó de saída para todos os nós de entrada.\n",
    "        for n in nodes_entrada:\n",
    "            n.nodes_saida.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Todo o nó que usar essa classe como uma classe base, precisa definir seu próprio método \"forward\".\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Todo o nó que usar essa classe como uma classe base, precisa definir seu próprio método \"backward\".\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "class Input(Neuronio):\n",
    "    \"\"\"\n",
    "    Input genérico para a rede.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # O construtor da classe base deve ser executado para configurar todas as propriedades aqui.\n",
    "        #\n",
    "        # A propriedade mais importante de Input é valor.\n",
    "        # self.valor é definido na função topological_sort().\n",
    "        Neuronio.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Nada a ser feito aqui.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # Um nó de Input não possui entradas (pois ele já é a entrada) e assim o gradiente (derivada) é zero.\n",
    "        # A palavra reservada \"self\", é referência para este objeto.\n",
    "        self.gradientes = {self: 0}\n",
    "        \n",
    "        # Pesos e bias podem ser inputs, assim precisamos somar o gradiente de outros gradientes de saída\n",
    "        for n in self.nodes_saida:\n",
    "            self.gradientes[self] += n.gradientes[self]\n",
    "            \n",
    "\n",
    "class Linear(Neuronio):\n",
    "    \"\"\"\n",
    "    Representa um nó que realiza transformação linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # O construtor da classe base (nó). \n",
    "        # Pesos e bias são tratados como nós de entrada (nodes_entrada).\n",
    "        Neuronio.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Executa a matemática por trás da transformação linear.\n",
    "        \"\"\"\n",
    "        X = self.nodes_entrada[0].valor\n",
    "        W = self.nodes_entrada[1].valor\n",
    "        b = self.nodes_entrada[2].valor\n",
    "        self.valor = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calcula o gradiente com base nos valores de saída.\n",
    "        \"\"\"\n",
    "        # Inicializa um parcial para cada um dos nodes_entrada.\n",
    "        self.gradientes = {n: np.zeros_like(n.valor) for n in self.nodes_entrada}\n",
    "        \n",
    "        # Ciclo através dos outputs. \n",
    "        # O gradiente mudará dependendo de cada output, assim os gradientes são somados sobre todos os outputs.\n",
    "        for n in self.nodes_saida:\n",
    "            \n",
    "            # Obtendo parcial da perda em relação a este nó.\n",
    "            grad_cost = n.gradientes[self]\n",
    "            \n",
    "            # Definindo o parcial da perda em relação às entradas deste nó.\n",
    "            self.gradientes[self.nodes_entrada[0]] += np.dot(grad_cost, self.nodes_entrada[1].valor.T)\n",
    "            \n",
    "            # Definindo o parcial da perda em relação aos pesos deste nó.\n",
    "            self.gradientes[self.nodes_entrada[1]] += np.dot(self.nodes_entrada[0].valor.T, grad_cost)\n",
    "            \n",
    "            # Definindo o parcial da perda em relação ao bias deste nó.\n",
    "            self.gradientes[self.nodes_entrada[2]] += np.sum(grad_cost, axis = 0, keepdims = False)\n",
    "\n",
    "\n",
    "class Sigmoid(Neuronio):\n",
    "    \"\"\"\n",
    "    Representa o nó da função de ativação Sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # O construtor da classe base.\n",
    "        Neuronio.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Este método é separado do `forward` porque ele também será usado com \"backward\".\n",
    "\n",
    "        `x`: Um array Numpy.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Executa a função _sigmoid e define a variável self.valor\n",
    "        \"\"\"\n",
    "        input_value = self.nodes_entrada[0].valor\n",
    "        self.valor = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calcula o gradiente usando a derivada da função sigmoid \n",
    "        \n",
    "        O método backward da classe Sigmoid, soma as derivadas (é uma derivada normal quando há apenas uma variável) \n",
    "        em relação à única entrada sobre todos os nós de saída.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializa os gradientes com zero.\n",
    "        self.gradientes = {n: np.zeros_like(n.valor) for n in self.nodes_entrada}\n",
    "        \n",
    "        # Soma a parcial em relação ao input sobre todos os outputs.\n",
    "        for n in self.nodes_saida:\n",
    "            grad_cost = n.gradientes[self]\n",
    "            sigmoid = self.valor\n",
    "            self.gradientes[self.nodes_entrada[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Neuronio):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        Função de custo para calcular o erro médio quadrático.\n",
    "        Deve ser usado como último nó da rede.\n",
    "        \"\"\"\n",
    "        # Chamada ao construtor da classe base.\n",
    "        Neuronio.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calcula o erro médio ao quadrado.\n",
    "        \"\"\"\n",
    "        # Fazemos o reshape para evitar possíveis problemas nas operações de matrizes/vetores \n",
    "        #\n",
    "        # Convertendo os 2 arrays (3,1) garantimos que o resultado será (3,1) e, assim, \n",
    "        # teremos uma subtração elementwise.\n",
    "        y = self.nodes_entrada[0].valor.reshape(-1, 1)\n",
    "        a = self.nodes_entrada[1].valor.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.nodes_entrada[0].valor.shape[0]\n",
    "        \n",
    "        # Salva o output computado para o backward pass.\n",
    "        self.diff = y - a\n",
    "        self.valor = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calcula o gradiente do custo.\n",
    "        \"\"\"\n",
    "        self.gradientes[self.nodes_entrada[0]] = (2 / self.m) * self.diff\n",
    "        self.gradientes[self.nodes_entrada[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Classifica os nós em ordem topológica usando o Algoritmo de Kahn.\n",
    "\n",
    "    `Feed_dict`: um dicionário em que a chave é um nó `Input` e o valor é o respectivo feed de valor para esse nó.\n",
    "\n",
    "    Retorna uma lista de nós ordenados.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.nodes_saida:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.valor = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.nodes_saida:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Executa uma passagem para a frente e uma passagem para trás através de uma lista de nós ordenados.\n",
    "\n",
    "     Argumentos:\n",
    "\n",
    "         `Graph`: O resultado de `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # O valor negativo no slice permite fazer uma cópia da mesma lista na ordem inversa.\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(params, learning_rate = 1e-2):\n",
    "    \"\"\"\n",
    "    Atualiza o valor de cada parâmetro treinável com o SGD.\n",
    "\n",
    "    Argumentos:\n",
    "\n",
    "         `Trainables`: uma lista de nós `Input` que representam pesos / bias.\n",
    "         `Learning_rate`: a taxa de aprendizado.\n",
    "    \"\"\"\n",
    "    # Executa o SGD\n",
    "    #\n",
    "    # Loop sobre todos os parâmetros\n",
    "    for t in params:\n",
    "        # Alterar o valor do parâmetro, subtraindo a taxa de aprendizado \n",
    "        # multiplicado pela parte do custo em relação a esse parâmetro\n",
    "        partial = t.gradientes[t]\n",
    "        t.valor -= learning_rate * partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executando o Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados\n",
    "X_, y_ = load_boston(return_X_y=True)\n",
    "print(f'variaveis preditoras/esplicativas tem o shape de: {X_.shape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza os dados\n",
    "X_ = (X_ - np.mean(X_, axis = 0)) / np.std(X_, axis = 0)\n",
    "\n",
    "# Número de features e número de neurônios\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define valores randômicos para inicializar pesos e bias\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)"
   ]
  },
  {
   "source": [
    "### Antes de seguir, quem consegue explicar o que está ocorrendo acima?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede Neural\n",
    "# Prestem atenção no que faz a classe Input assim como as carácteristicas de Herança e sobrecarga\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "# Define o feed_dict\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de epochs (altere esse valor para ver as mudanças no resultado)\n",
    "epochs = 1000\n",
    "\n",
    "# Número total de exemplos\n",
    "m = X_.shape[0]\n",
    "\n",
    "# Batch size\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o grafo computacional\n",
    "graph = topological_sort(feed_dict)\n",
    "#for j in graph:\n",
    "#    print(j.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Valores que serão aprendidos pela rede\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "# Número total de exemplos\n",
    "print(\"Número Total de Exemplos = {}\".format(m))\n",
    "\n",
    "cost = []\n",
    "\n",
    "# Treinamento do modelo\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        \n",
    "        # Passo 1 - Testa aleatoriamente um lote de exemplos\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples = batch_size)\n",
    "\n",
    "        # Reset dos valores de X e y \n",
    "        X.valor = X_batch\n",
    "        y.valor = y_batch\n",
    "\n",
    "        # Passo 2 - Forward e Backpropagation\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Passo 3 - Otimização por SGD\n",
    "        sgd_update(params)\n",
    "\n",
    "        loss += graph[-1].valor\n",
    "    cost.append(loss/steps_per_epoch)\n",
    "\n",
    "    print(\"Epoch: {}, Custo: {:.3f}\".format(i+1, loss/steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(cost)+1), cost, marker = 'o')\n",
    "plt.title('Gráfico de loss x epoc')\n",
    "plt.xlabel('Iterações')\n",
    "plt.ylabel('Sum Squared Error - SSE')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Mão na massa -> implemente esse modelo com Gradient Descent (GD) no lugar de Stochastic Gradient Descent (SGD)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}